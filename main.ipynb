{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0500ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, Embedding, Layer, MultiHeadAttention, Add, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    " \n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c921b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('data/headlines.csv')\n",
    "scores = pd.read_csv('data/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e6eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 663 ms, total: 28.2 s\n",
      "Wall time: 28.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(418081, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "numberbatch_embeddings = load_embeddings(\"embeddings/numberbatch-en-17.04b.txt\")\n",
    "numberbatch_embeddings.shape\n",
    "\n",
    "EMBED_SIZE = numberbatch_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a2cb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_headlines = pd.read_csv('data/labeled_headlines.csv')\n",
    "\n",
    "X = labeled_headlines['title'].astype(str).values\n",
    "y = labeled_headlines['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_TITLE_LEN = 32 # maximum and minimum number of words\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_TITLE_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y, num_classes=2)\n",
    "\n",
    "print(f\"Found {len(word_index):,} unique tokens. Distilled to {top_words:,} top words.\")\n",
    "\n",
    "\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "# keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = numberbatch_embeddings.loc[word].values\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(f\"Embedding Shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total words found: {found_words:,}\")\n",
    "print(f\"Percentage: {round(100 * found_words / embedding_matrix.shape[0], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2, stratify=y)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train_ohe shape: {y_train_ohe.shape}\")\n",
    "print(f\"y_test_ohe shape: {y_test_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_TITLE_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, embedding_dim, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        positions = tf.cast(positions, tf.float32)\n",
    "        positions = tf.expand_dims(positions, axis=-1)\n",
    "        pos_encoding = inputs + tf.math.sin(positions / (10000 ** (2 * tf.range(0, self.embedding_dim, 2) / self.embedding_dim)))\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "num_trans = 6\n",
    "\n",
    "inputs = Input(shape=(max_len,), dtype=tf.int32)\n",
    "embedding = embedding_layer(inputs)\n",
    "pos_encoding = PositionalEncoding(embedding)\n",
    "x = pos_encoding(inputs)\n",
    "\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "for i in range(num_trans): x = transformer_block(x) \n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with mean squared error loss and Adam optimizer\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss=\"mse\", metrics=[\"accuracy\"])\n",
    " \n",
    "# Train the model\n",
    "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), batch_size=256, epochs=10)\n",
    "# TODO: see how big we can make the batch size\n",
    "# probably very large when using the V100's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
