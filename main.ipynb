{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0500ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    " \n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c921b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('data/headlines.csv')\n",
    "scores = pd.read_csv('data/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e6eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 663 ms, total: 28.2 s\n",
      "Wall time: 28.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(418081, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "numberbatch_embeddings = load_embeddings(\"embeddings/numberbatch-en-17.04b.txt\")\n",
    "numberbatch_embeddings.shape\n",
    "\n",
    "EMBED_SIZE = numberbatch_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a2cb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_headlines = pd.read_csv('data/labeled_headlines.csv')\n",
    "\n",
    "X = labeled_headlines['title'].astype(str).values\n",
    "y = labeled_headlines['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_TITLE_LEN = 32 # maximum and minimum number of words\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_TITLE_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y, num_classes=2)\n",
    "\n",
    "print(f\"Found {len(word_index):,} unique tokens. Distilled to {top_words:,} top words.\")\n",
    "\n",
    "\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "# keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = numberbatch_embeddings.loc[word].values\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(f\"Embedding Shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total words found: {found_words:,}\")\n",
    "print(f\"Percentage: {round(100 * found_words / embedding_matrix.shape[0], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2, stratify=y)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train_ohe shape: {y_train_ohe.shape}\")\n",
    "print(f\"y_test_ohe shape: {y_test_ohe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_TITLE_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, max_len, embedding_dim, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        positions = tf.cast(positions, tf.float32)\n",
    "        positions = tf.expand_dims(positions, axis=-1)\n",
    "        pos_encoding = inputs + tf.math.sin(positions / (10000 ** (2 * tf.range(0, self.embedding_dim, 2) / self.embedding_dim)))\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the hyperparameters\n",
    "max_len = MAX_TITLE_LEN  # Maximum sequence length for padding\n",
    "num_classes = 10  # Number of classes for classification\n",
    "embedding_dim = 100  # Dimension of the word embedding vectors\n",
    "num_heads = 2  # Number of attention heads in each transformer layer\n",
    "num_transformer_layers = 2  # Number of transformer layers in the model\n",
    "hidden_units = 64  # Number of hidden units in the feedforward layers\n",
    " \n",
    "inputs = Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "pos_encoding = PositionalEncoding(max_len, embedding_dim)(embedding)\n",
    "\n",
    "# Define the transformer layers\n",
    "for i in range(num_transformer_layers):\n",
    "    # Multi-head self-attention layer\n",
    "    multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "    attention_output = multi_head_attention(embedding, embedding)\n",
    "    attention_output = tf.keras.layers.Dropout(0.1)(attention_output)\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + embedding)\n",
    "\n",
    "    # Feedforward layer\n",
    "    feedforward = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(embedding_dim)\n",
    "    ])\n",
    "    \n",
    "    feedforward_output = feedforward(attention_output)\n",
    "    feedforward_output = tf.keras.layers.Dropout(0.1)(feedforward_output)\n",
    "    transformer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + feedforward_output)\n",
    "\n",
    "    # Update the positional encoding\n",
    "    pos_encoding = PositionalEncoding(max_len, embedding_dim)(transformer_output)\n",
    " \n",
    "# Global average pooling layer\n",
    "pooling_layer = GlobalAveragePooling1D()\n",
    "pooling_output = pooling_layer(transformer_output)\n",
    " \n",
    "# Dense layer with dropout\n",
    "dense_layer = Dense(64, activation='relu')(pooling_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    " \n",
    "# Output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(dropout_layer)\n",
    " \n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Train the model\n",
    "model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), batch_size=256, epochs=10)\n",
    "# TODO: see how big we can make the batch size\n",
    "# probably very large when using the V100's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
