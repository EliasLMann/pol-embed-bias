{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0500ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    " \n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c921b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('data/headlines.csv')\n",
    "scores = pd.read_csv('data/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e6eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8341a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 663 ms, total: 28.2 s\n",
      "Wall time: 28.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(418081, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "numberbatch_embeddings = load_embeddings(\"embeddings/numberbatch-en-17.04b.txt\")\n",
    "numberbatch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a2cb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_headlines = pd.read_csv('data/labeled_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# Define the hyperparameters\n",
    "max_len = 500  # Maximum sequence length for padding\n",
    "vocab_size = 5000  # Maximum number of words to keep based on word frequency\n",
    "num_classes = 10  # Number of classes for classification\n",
    "embedding_dim = 100  # Dimension of the word embedding vectors\n",
    "num_heads = 2  # Number of attention heads in each transformer layer\n",
    "num_transformer_layers = 2  # Number of transformer layers in the model\n",
    "hidden_units = 64  # Number of hidden units in the feedforward layers\n",
    " \n",
    "\n",
    "\n",
    "# Define the transformer layers\n",
    "for i in range(num_transformer_layers):\n",
    "    # Multi-head self-attention layer\n",
    "    multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "    attention_output = multi_head_attention(embedding, embedding)\n",
    "    attention_output = tf.keras.layers.Dropout(0.1)(attention_output)\n",
    "    #attention_output = tf.keras.layers.LayerNormalization(epsilon=\n",
    "    # Feedforward layer\n",
    "    feedforward = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(embedding_dim)\n",
    "    ])\n",
    "    feedforward_output = feedforward(attention_output)\n",
    "    feedforward_output = tf.keras.layers.Dropout(0.1)(feedforward_output)\n",
    "    transformer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + feedforward_output)\n",
    " \n",
    "# Global average pooling layer\n",
    "pooling_layer = GlobalAveragePooling1D()\n",
    "pooling_output = pooling_layer(transformer_output)\n",
    " \n",
    "# Dense layer with dropout\n",
    "dense_layer = Dense(64, activation='relu')(pooling_output)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    " \n",
    "# Output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(dropout_layer)\n",
    " \n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
